{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## Installation (CAN SKIP ONLY NEED TO INSTALL ONCE INTO THIS NOTEBOOK INSTANCE)\n",
    "\n",
    "Install the latest version of Vertex AI SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "VRJ1PfqmMEYN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # Google Cloud Notebook\n",
    "# if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "#     USER_FLAG = \"--user\"\n",
    "# else:\n",
    "#     USER_FLAG = \"\"\n",
    "\n",
    "# ! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_storage"
   },
   "source": [
    "Install the latest GA version of *google-cloud-storage* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "W1iPNVpBMEYP"
   },
   "outputs": [],
   "source": [
    "# ! pip3 install -U google-cloud-storage $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_gcpc"
   },
   "source": [
    "Install the latest GA version of *google-cloud-pipeline-components* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "Hln-WFVCMEYQ"
   },
   "outputs": [],
   "source": [
    "# ! pip3 install $USER kfp google-cloud-pipeline-components --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "l3G-qfTNMEYR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     # Automatically restart kernel after installs\n",
    "#     import IPython\n",
    "\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_versions"
   },
   "source": [
    "Check the versions of the packages you installed.  The KFP SDK version should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "check_versions:kfp,gcpc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.12\n",
      "google_cloud_pipeline_components version: 1.0.8\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "## START RUNNING HERE\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### GPU runtime\n",
    "\n",
    "This tutorial does not require a GPU runtime.\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [Enable the Vertex AI APIs, Compute Engine APIs, and Cloud Storage.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,storage-component.googleapis.com)\n",
    "\n",
    "4. [The Google Cloud SDK](https://cloud.google.com/sdk) is already installed in Google Cloud Notebook.\n",
    "\n",
    "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"daring-hash-348101\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "# this step causes bash to ask for reply, which cannot be done in notebook. So use cloud shell instead\n",
    "# but no need to do it if project is already set to the right one\n",
    "\n",
    "# ! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UwhC3m6KMEYT"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-east1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2rRvTXjmMEYT"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"seangoh-smu-mle-usa\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "BgWYzAIzMEYU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: \"mb\" command does not support \"file://\" URLs. Did you mean to use a gs:// URL?\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cNsCW4ArMEYV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://seangoh-smu-mle-usa/FaceMask/\n",
      "                                 gs://seangoh-smu-mle-usa/Models/\n",
      "                                 gs://seangoh-smu-mle-usa/logs/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### Service Account\n",
    "\n",
    "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below.\n",
    "\n",
    "**LEAVE THIS AS `[your-service-account]` AND LET GCLOUD COMMAND FIGURE IT OUT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ctALLIOzMEYV"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 591661299323-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud auth list 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[2].replace('*', '').strip()\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D9xLjQ6vMEYV"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "from google_cloud_pipeline_components.v1.model import \\\n",
    "    ModelUploadOp as model_upload_op\n",
    "\n",
    "import kfp\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.components import importer_node\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "from typing import NamedTuple\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline_constants"
   },
   "source": [
    "#### Vertex AI Pipelines constants\n",
    "\n",
    "Setup up the following constants for Vertex AI Pipelines:\n",
    "\n",
    "`PIPELINE_ROOT`: The artifact repository where KFP stores a pipeline’s artifacts. <br>\n",
    "`CONTAINER_IMG_URI`: Container image for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "kY6vhNQ7MEYW"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/face-mask\".format(BUCKET_URI)\n",
    "# CONTAINER_IMG_URI = 'us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/efficientnettrain:latest'\n",
    "CONTAINER_IMG_URI = 'us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/mobilenettrain:latest'\n",
    "MODEL_URI = \"gs://seangoh-smu-mle-usa/Models/FaceMaskMobileNetModel/\" # change to cloud storage stored model location\n",
    "DISPLAY_NAME = \"face-mask-classification-pipeline_\" + TIMESTAMP\n",
    "TEST_URI = \"\" # change to cloud storage test dataset location\n",
    "MODEL_DISPLAY_NAME = f\"face-mask-classification-{TIMESTAMP}\"\n",
    "THRESHOLDS_DICT_STR = \"{'f1': 0.8}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "## Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "hYEEaYkDMEYW"
   },
   "outputs": [],
   "source": [
    "aip.init(\n",
    "    project=PROJECT_ID, \n",
    "    staging_bucket=BUCKET_URI,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_pipeline:gcpc,automl,flowers,icn"
   },
   "source": [
    "## Define image classification model pipeline that uses components from `google_cloud_pipeline_components` and custom components\n",
    "\n",
    "Next, you define the pipeline.\n",
    "\n",
    "<img alt=\"alt_text\" width=\"1000px\" src=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/pytorch_text_classification_using_vertex_sdk_and_gcloud/images/pipelines-high-level-flow.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline",
    "tags": []
   },
   "source": [
    "## Create the pipeline\n",
    "\n",
    "Next, create the pipeline.\n",
    "\n",
    "<img alt=\"alt_text\" width=\"1000px\" src=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/pytorch_text_classification_using_vertex_sdk_and_gcloud/images/concepts-of-a-pipeline.png?raw=true\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline",
    "tags": []
   },
   "source": [
    "#### This is new and experimental `custom_job_wrapper` to run any component as a custom component. Can't get it to work so don't run this portion. Skip to the next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CREATE PIPELINE\n",
    "# from google_cloud_pipeline_components.experimental import custom_job\n",
    "\n",
    "# @kfp.dsl.pipeline(\n",
    "#     name=\"face-mask-pipeline-v1\",\n",
    "#     pipeline_root=PIPELINE_ROOT,\n",
    "# )\n",
    "# def pipeline(\n",
    "#     project: str, \n",
    "#     region: str,\n",
    "#     service_account: str,\n",
    "#     container_img_uri: str\n",
    "# ):\n",
    "    \n",
    "#     # RUN TRAINING (CUSTOM COMPONENT FOR RUNNING CONTAINER)\n",
    "    \n",
    "#     # ========================================================================\n",
    "#     # model training\n",
    "#     # ========================================================================\n",
    "#     # train the model on Vertex AI by submitting a CustomJob\n",
    "#     # using the custom container (no hyper-parameter tuning)\n",
    "#     # define training code arguments\n",
    "#     # training_args = [\"--num-epochs\", \"2\", \"--model-name\", cfg.MODEL_NAME]\n",
    "    \n",
    "#     # define job name\n",
    "#     JOB_NAME = f\"face-mask-train-{TIMESTAMP}\"\n",
    "#     GCS_BASE_OUTPUT_DIR = f\"{PIPELINE_ROOT}/mobilenett/\"\n",
    "    \n",
    "#     # define worker pool specs\n",
    "#     worker_pool_specs = [\n",
    "#         {\n",
    "#             \"machine_spec\": {\n",
    "#                 \"machine_type\": 'n1-standard-8',\n",
    "#                 \"accelerator_type\": 'NVIDIA_TESLA_K80',\n",
    "#                 \"accelerator_count\": '1',\n",
    "#             },\n",
    "#             \"replica_count\": '1',\n",
    "#             \"container_spec\": {\"image_uri\": f'{container_img_uri}'},\n",
    "#         }\n",
    "#     ]\n",
    "    \n",
    "    \n",
    "#     run_train_task = (\n",
    "#         custom_job.CustomTrainingJobOp(\n",
    "#             project=f'{project}',\n",
    "#             location=f'{region}',\n",
    "#             display_name=JOB_NAME,\n",
    "#             base_output_directory=GCS_BASE_OUTPUT_DIR,\n",
    "#             worker_pool_specs=worker_pool_specs,\n",
    "#         )\n",
    "#         .set_display_name(\"Run custom training job\")\n",
    "#         .after(build_custom_train_image_task)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline"
   },
   "source": [
    "## Compile the pipeline\n",
    "\n",
    "Next, compile the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "I_DPbLeCMEYX"
   },
   "outputs": [],
   "source": [
    "# from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "# compiler.Compiler().compile(\n",
    "#     pipeline_func=pipeline,\n",
    "#     package_path=\"image classification_pipeline.json\".replace(\" \", \"_\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:automl,image"
   },
   "source": [
    "## Run the pipeline\n",
    "\n",
    "Next, run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jqPb1QHkMEYX"
   },
   "outputs": [],
   "source": [
    "# pipeline_params = {\n",
    "#     \"project\": PROJECT_ID,\n",
    "#     \"region\": REGION,\n",
    "#     \"service_account\": SERVICE_ACCOUNT,\n",
    "#     \"container_img_uri\": CONTAINER_IMG_URI,\n",
    "# }\n",
    "# DISPLAY_NAME = \"face_mask_pipeline_\" + TIMESTAMP\n",
    "\n",
    "# aip.init(\n",
    "#     project=PROJECT_ID,\n",
    "#     staging_bucket=BUCKET_URI,\n",
    "#     location=REGION,\n",
    "# )\n",
    "\n",
    "# job = aip.PipelineJob(\n",
    "#     display_name=DISPLAY_NAME,\n",
    "#     template_path=\"image classification_pipeline.json\".replace(\" \", \"_\"),\n",
    "#     pipeline_root=PIPELINE_ROOT,\n",
    "#     enable_caching=False,\n",
    "#     parameter_values=pipeline_params,\n",
    "# )\n",
    "# job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline",
    "tags": []
   },
   "source": [
    "#### USE THIS PORTION. This method is going to be deprecated by GCP but it works for now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline",
    "tags": []
   },
   "source": [
    "## Create the pipeline\n",
    "\n",
    "Next, create the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,\n",
    "                                                              ModelDeployOp)\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=BUCKET_URI,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"face-mask-import-model-v1\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline(\n",
    "    display_name:str,\n",
    "    project: str, \n",
    "    region: str,\n",
    "    bucket: str,\n",
    "    container_img_uri: str,\n",
    "    artifact_uri: str,\n",
    "):\n",
    "    \n",
    "    # custom model training for custom container\n",
    "    training_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        display_name=display_name,\n",
    "        container_uri=container_img_uri,\n",
    "        project=project,\n",
    "        location=region,\n",
    "        staging_bucket=bucket,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        replica_count=1,\n",
    "        accelerator_type='NVIDIA_TESLA_K80',\n",
    "        accelerator_count=1,  \n",
    "    )\n",
    "    \n",
    "    # import the model from GCS\n",
    "    import_unmanaged_model_task = importer_node.importer(\n",
    "            artifact_uri=artifact_uri,\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-gpu.2-8:latest\",\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "    import_unmanaged_model_task.after(training_op)\n",
    "    \n",
    "    # upload the model to VertexAI Model Registry\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=project,\n",
    "        location=region,\n",
    "        display_name=display_name,\n",
    "        unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "    )\n",
    "    model_upload_op.after(import_unmanaged_model_task)\n",
    "\n",
    "    # create Endpoint. This is run in parallel as it is not dependent on previous nodes\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=project,\n",
    "        location=region,\n",
    "        display_name=\"pipelines-created-endpoint\",\n",
    "    )\n",
    "    \n",
    "    # deploy the model from Model Registry\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=display_name,\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        dedicated_resources_accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "        dedicated_resources_accelerator_count=1,\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline"
   },
   "source": [
    "## Compile the pipeline\n",
    "\n",
    "Next, compile the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"facemask classification pipeline.json\".replace(\" \", \"_\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:automl,image"
   },
   "source": [
    "## Run the pipeline\n",
    "\n",
    "Next, run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-east1/pipelines/runs/face-mask-import-model-v1-20220613112911?project=591661299323\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220613112911\n"
     ]
    }
   ],
   "source": [
    "pipeline_params = {\n",
    "    \"display_name\": DISPLAY_NAME,\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"region\": REGION,\n",
    "    \"bucket\": BUCKET_URI,\n",
    "    \"container_img_uri\": CONTAINER_IMG_URI,\n",
    "    \"artifact_uri\": MODEL_URI,\n",
    "}\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"facemask classification pipeline.json\".replace(\" \", \"_\"),\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    "    parameter_values=pipeline_params,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipeline_run:automl,image"
   },
   "source": [
    "Click on the generated link to see your run in the Cloud Console.\n",
    "\n",
    "<!-- It should look something like this as it is running:\n",
    "\n",
    "<a href=\"https://storage.googleapis.com/amy-jo/images/mp/automl_tabular_classif.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/mp/automl_tabular_classif.png\" width=\"40%\"/></a> -->\n",
    "\n",
    "In the UI, many of the pipeline DAG nodes will expand or collapse when you click on them. Here is a partially-expanded view of the DAG (click image to see larger version).\n",
    "\n",
    "<a href=\"https://storage.googleapis.com/amy-jo/images/mp/automl_image_classif.png\" target=\"_blank\"><img src=\"https://storage.googleapis.com/amy-jo/images/mp/automl_image_classif.png\" width=\"40%\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:pipelines"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial -- *Note:* this is auto-generated and not all resources may be applicable for this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- Pipeline\n",
    "- Model\n",
    "- Endpoint\n",
    "- Batch Job\n",
    "- Custom Job\n",
    "- Hyperparameter Tuning Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHloY5skMEYY"
   },
   "outputs": [],
   "source": [
    "# delete_dataset = True\n",
    "# delete_pipeline = True\n",
    "# delete_model = True\n",
    "# delete_endpoint = True\n",
    "# delete_batchjob = True\n",
    "# delete_customjob = True\n",
    "# delete_hptjob = True\n",
    "# delete_bucket = True\n",
    "\n",
    "# try:\n",
    "#     if delete_model and \"DISPLAY_NAME\" in globals():\n",
    "#         models = aip.Model.list(\n",
    "#             filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "#         )\n",
    "#         model = models[0]\n",
    "#         aip.Model.delete(model)\n",
    "#         print(\"Deleted model:\", model)\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "\n",
    "# try:\n",
    "#     if delete_endpoint and \"DISPLAY_NAME\" in globals():\n",
    "#         endpoints = aip.Endpoint.list(\n",
    "#             filter=f\"display_name={DISPLAY_NAME}_endpoint\", order_by=\"create_time\"\n",
    "#         )\n",
    "#         endpoint = endpoints[0]\n",
    "#         endpoint.undeploy_all()\n",
    "#         aip.Endpoint.delete(endpoint.resource_name)\n",
    "#         print(\"Deleted endpoint:\", endpoint)\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "\n",
    "# if delete_dataset and \"DISPLAY_NAME\" in globals():\n",
    "#     if \"image\" == \"tabular\":\n",
    "#         try:\n",
    "#             datasets = aip.TabularDataset.list(\n",
    "#                 filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "#             )\n",
    "#             dataset = datasets[0]\n",
    "#             aip.TabularDataset.delete(dataset.resource_name)\n",
    "#             print(\"Deleted dataset:\", dataset)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "\n",
    "#     if \"image\" == \"image\":\n",
    "#         try:\n",
    "#             datasets = aip.ImageDataset.list(\n",
    "#                 filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "#             )\n",
    "#             dataset = datasets[0]\n",
    "#             aip.ImageDataset.delete(dataset.resource_name)\n",
    "#             print(\"Deleted dataset:\", dataset)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "\n",
    "#     if \"image\" == \"text\":\n",
    "#         try:\n",
    "#             datasets = aip.TextDataset.list(\n",
    "#                 filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "#             )\n",
    "#             dataset = datasets[0]\n",
    "#             aip.TextDataset.delete(dataset.resource_name)\n",
    "#             print(\"Deleted dataset:\", dataset)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "\n",
    "#     if \"image\" == \"video\":\n",
    "#         try:\n",
    "#             datasets = aip.VideoDataset.list(\n",
    "#                 filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "#             )\n",
    "#             dataset = datasets[0]\n",
    "#             aip.VideoDataset.delete(dataset.resource_name)\n",
    "#             print(\"Deleted dataset:\", dataset)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "\n",
    "# try:\n",
    "#     if delete_pipeline and \"DISPLAY_NAME\" in globals():\n",
    "#         pipelines = aip.PipelineJob.list(\n",
    "#             filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "#         )\n",
    "#         pipeline = pipelines[0]\n",
    "#         aip.PipelineJob.delete(pipeline.resource_name)\n",
    "#         print(\"Deleted pipeline:\", pipeline)\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "\n",
    "# if delete_bucket and \"BUCKET_NAME\" in globals():\n",
    "#     ! gsutil rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "google_cloud_pipeline_components_automl_images.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
