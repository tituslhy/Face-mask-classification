{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## Installation (CAN SKIP ONLY NEED TO INSTALL ONCE INTO THIS NOTEBOOK INSTANCE)\n",
    "\n",
    "Install the latest version of Vertex AI SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VRJ1PfqmMEYN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # Google Cloud Notebook\n",
    "# if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "#     USER_FLAG = \"--user\"\n",
    "# else:\n",
    "#     USER_FLAG = \"\"\n",
    "\n",
    "# ! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_storage"
   },
   "source": [
    "Install the latest GA version of *google-cloud-storage* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "W1iPNVpBMEYP"
   },
   "outputs": [],
   "source": [
    "# ! pip3 install -U google-cloud-storage $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_gcpc"
   },
   "source": [
    "Install the latest GA version of *google-cloud-pipeline-components* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Hln-WFVCMEYQ"
   },
   "outputs": [],
   "source": [
    "# ! pip3 install $USER kfp google-cloud-pipeline-components --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l3G-qfTNMEYR"
   },
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_versions"
   },
   "source": [
    "Check the versions of the packages you installed.  The KFP SDK version should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "check_versions:kfp,gcpc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.12\n",
      "google_cloud_pipeline_components version: 1.0.10\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "## START RUNNING HERE\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### GPU runtime\n",
    "\n",
    "This tutorial does not require a GPU runtime.\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [Enable the Vertex AI APIs, Compute Engine APIs, and Cloud Storage.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component,storage-component.googleapis.com)\n",
    "\n",
    "4. [The Google Cloud SDK](https://cloud.google.com/sdk) is already installed in Google Cloud Notebook.\n",
    "\n",
    "5. Enter your project ID in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"daring-hash-348101\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [],
   "source": [
    "# this step causes bash to ask for reply, which cannot be done in notebook. So use cloud shell instead\n",
    "# but no need to do it if project is already set to the right one\n",
    "\n",
    "# ! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UwhC3m6KMEYT"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-east1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2rRvTXjmMEYT"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex AI SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"seangoh-smu-mle-usa\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BgWYzAIzMEYU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://seangoh-smu-mle-usa/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'seangoh-smu-mle-usa' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cNsCW4ArMEYV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://seangoh-smu-mle-usa/DailyQC/\n",
      "                                 gs://seangoh-smu-mle-usa/FaceMask/\n",
      "                                 gs://seangoh-smu-mle-usa/Models/\n",
      "                                 gs://seangoh-smu-mle-usa/ProjectPipeline/\n",
      "                                 gs://seangoh-smu-mle-usa/assignment2/\n",
      "                                 gs://seangoh-smu-mle-usa/logs/\n",
      "                                 gs://seangoh-smu-mle-usa/pipeline_root/\n",
      "                                 gs://seangoh-smu-mle-usa/testupload/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### Service Account\n",
    "\n",
    "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below.\n",
    "\n",
    "**LEAVE THIS AS `[your-service-account]` AND LET GCLOUD COMMAND FIGURE IT OUT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ctALLIOzMEYV"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 591661299323-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud auth list 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[2].replace('*', '').strip()\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D9xLjQ6vMEYV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes made to gs://seangoh-smu-mle-usa/\n",
      "No changes made to gs://seangoh-smu-mle-usa/\n"
     ]
    }
   ],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.experimental.custom_job import utils\n",
    "# from google_cloud_pipeline_components.v1.model import \\\n",
    "#     ModelUploadOp as model_upload_op\n",
    "from google_cloud_pipeline_components.aiplatform import ModelUploadOp, ModelDeployOp, EndpointCreateOp\n",
    "from google_cloud_pipeline_components.experimental.vertex_notification_email import VertexNotificationEmailOp\n",
    "\n",
    "import kfp\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.components import importer_node\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "from typing import NamedTuple\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pipeline_constants"
   },
   "source": [
    "#### Vertex AI Pipelines constants\n",
    "\n",
    "Setup up the following constants for Vertex AI Pipelines:\n",
    "\n",
    "`PIPELINE_ROOT`: The artifact repository where KFP stores a pipelineâ€™s artifacts. <br>\n",
    "`CONTAINER_IMG_URI`: Container image for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kY6vhNQ7MEYW"
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/face-mask\".format(BUCKET_URI)\n",
    "DATA_DIR = 'gs://seangoh-smu-mle-usa/FaceMask/'\n",
    "DIVERGENCE_THRESHOLD = 0.01\n",
    "# CONTAINER_IMG_URI = 'us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/efficientnettrain:latest'\n",
    "CONTAINER_IMG_URI = 'us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/mobilenettrain:latest'\n",
    "MODEL_URI = \"gs://seangoh-smu-mle-usa/Models/FaceMaskMobileNetModel/\" # change to cloud storage stored model location\n",
    "DISPLAY_NAME = \"face-mask-classification-pipeline_\" + TIMESTAMP\n",
    "TEST_URI = \"\" # change to cloud storage test dataset location\n",
    "MODEL_DISPLAY_NAME = f\"face-mask-classification-{TIMESTAMP}\"\n",
    "MODEL_PERFORMANCE_FILENAME = \"MobileNetPerformanceComparison.json\"\n",
    "THRESHOLDS_DICT = {'f1': 0.9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "## Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hYEEaYkDMEYW"
   },
   "outputs": [],
   "source": [
    "aip.init(\n",
    "    project=PROJECT_ID, \n",
    "    staging_bucket=BUCKET_URI,\n",
    "    location=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "define_pipeline:gcpc,automl,flowers,icn"
   },
   "source": [
    "## Define image classification model pipeline that uses components from `google_cloud_pipeline_components` and custom components\n",
    "\n",
    "Next, you define the pipeline.\n",
    "\n",
    "<img alt=\"alt_text\" width=\"1000px\" src=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/pytorch_text_classification_using_vertex_sdk_and_gcloud/images/pipelines-high-level-flow.png?raw=true\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline",
    "tags": []
   },
   "source": [
    "## Create the pipeline\n",
    "\n",
    "Next, create the pipeline.\n",
    "\n",
    "<img alt=\"alt_text\" width=\"1000px\" src=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/pytorch_text_classification_using_vertex_sdk_and_gcloud/images/concepts-of-a-pipeline.png?raw=true\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline",
    "tags": []
   },
   "source": [
    "### Data Validation Check\n",
    "\n",
    "Create custom component for checking that train and test set brightness distribution are not too dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-storage\", \"numpy\", \"matplotlib\", \"scipy\"],\n",
    "    output_component_file=\"data_validation.yaml\"\n",
    ")\n",
    "def DataValidationOp(\n",
    "    bucket_name: str,\n",
    "    divergence_threshold: float,\n",
    ") -> str:\n",
    "    from google.cloud import storage\n",
    "    import numpy as np\n",
    "    from matplotlib import image\n",
    "    from scipy.stats import wasserstein_distance\n",
    "    \n",
    "    def list_blobs(bucket_name):\n",
    "        \"\"\"Lists all the blobs in the bucket.\"\"\"\n",
    "        # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "        blobs = storage_client.list_blobs(bucket_name)\n",
    "\n",
    "        files = []\n",
    "\n",
    "        for blob in blobs:\n",
    "            files.append(blob.name)\n",
    "\n",
    "        return files\n",
    "\n",
    "    def get_image_brightness(bucket_name, source_blob_name, destination_file_name):\n",
    "        \"\"\"Downloads an image from the bucket and get its mean brightness.\"\"\"\n",
    "\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_file_name)\n",
    "\n",
    "        img = image.imread(destination_file_name)\n",
    "\n",
    "        return np.mean(img)\n",
    "\n",
    "    files = list_blobs(bucket_name)\n",
    "    train_files = [file for file in files if 'FaceMask/Train/' in file and 'Augmented' not in file]\n",
    "    val_files = [file for file in files if 'FaceMask/Validation/' in file and 'Augmented' not in file]\n",
    "    train_brightness = []\n",
    "    val_brightness = []\n",
    "\n",
    "    for file in train_files:\n",
    "        brightness = get_image_brightness(bucket_name, file, './image.png')\n",
    "        train_brightness.append(brightness)\n",
    "    for file in val_files:\n",
    "        brightness = get_image_brightness(bucket_name, file, './image.png')\n",
    "        val_brightness.append(brightness)    \n",
    "\n",
    "    # https://datascience.stackexchange.com/a/54385/73827\n",
    "    # wasserstein-1 distance is better than kl-divergence\n",
    "    divergence = wasserstein_distance(train_brightness, val_brightness)\n",
    "    if divergence > divergence_threshold:\n",
    "        return 'fail'\n",
    "    else:\n",
    "        return 'pass'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline",
    "tags": []
   },
   "source": [
    "### Model Performance Threshold Check\n",
    "\n",
    "Create custom component for checking that model performance on validation set passes a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-storage\", \"pandas\", \"fsspec\", \"gcsfs\"],\n",
    "    output_component_file=\"PerformanceThresholdOp.yaml\"\n",
    ")\n",
    "def PerformanceThresholdOp(\n",
    "    json_url: str,\n",
    "    f1_threshold: float,\n",
    ") -> str:\n",
    "    import pandas as pd\n",
    "    \n",
    "    results = pd.read_json(json_url, typ='series')\n",
    "    if results['FullModel'] >= f1_threshold:\n",
    "        return 'pass'\n",
    "    else:\n",
    "        return 'fail'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline",
    "tags": []
   },
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"face-mask-import-model-v1\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline(\n",
    "    display_name:str,\n",
    "    project: str, \n",
    "    region: str,\n",
    "    bucket: str,\n",
    "    bucket_name: str,\n",
    "    divergence_threshold: float,\n",
    "    container_img_uri: str,\n",
    "    model_perf_uri: str,\n",
    "    eval_threshold: float,\n",
    "    artifact_uri: str,\n",
    "):\n",
    "    \n",
    "    notify_email_task = VertexNotificationEmailOp(\n",
    "        recipients=['hy.lim.2021@mitb.smu.edu.sg',\n",
    "                   'teyang.lau.2021@mitb.smu.edu.sg',\n",
    "                   'sean.goh.2020@mitb.smu.edu.sg']\n",
    "    )\n",
    "    \n",
    "    with dsl.ExitHandler(notify_email_task):\n",
    "    \n",
    "        # data validation op\n",
    "        data_validation_op = DataValidationOp(bucket_name, divergence_threshold)\n",
    "\n",
    "        with dsl.Condition(\n",
    "            data_validation_op.output=='pass',\n",
    "            name='train-model'\n",
    "        ): \n",
    "            # custom model training for custom container\n",
    "            training_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "                display_name=display_name,\n",
    "                container_uri=container_img_uri,\n",
    "                project=project,\n",
    "                location=region,\n",
    "                staging_bucket=bucket,\n",
    "                machine_type=\"n1-standard-4\",\n",
    "                replica_count=1,\n",
    "                accelerator_type='NVIDIA_TESLA_K80',\n",
    "                accelerator_count=1,  \n",
    "            )\n",
    "\n",
    "        # evaluate performance passes threshold\n",
    "        model_evaluate_op = PerformanceThresholdOp(\n",
    "            model_perf_uri, \n",
    "            eval_threshold,\n",
    "        ).after(training_op)\n",
    "\n",
    "        # if evaluation passes threshold, import, upload, deploy to endpoint\n",
    "        with dsl.Condition(\n",
    "            model_evaluate_op.output==\"pass\",\n",
    "            name=\"deploy-model\",\n",
    "        ):\n",
    "            # import the model from GCS\n",
    "            import_unmanaged_model_task = importer_node.importer(\n",
    "                    artifact_uri=artifact_uri,\n",
    "                    artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "                    metadata={\n",
    "                        \"containerSpec\": {\n",
    "                            \"imageUri\": \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-gpu.2-8:latest\",\n",
    "                        },\n",
    "                    },\n",
    "                )\n",
    "            # import_unmanaged_model_task.after(training_op)\n",
    "\n",
    "            # upload the model to VertexAI Model Registry\n",
    "            model_upload_op = ModelUploadOp(\n",
    "                project=project,\n",
    "                location=region,\n",
    "                display_name=display_name,\n",
    "                unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "            ).after(import_unmanaged_model_task)\n",
    "\n",
    "            # create Endpoint. This is run in parallel as it is not dependent on previous nodes\n",
    "            endpoint_create_op = EndpointCreateOp(\n",
    "                project=project,\n",
    "                location=region,\n",
    "                display_name=\"pipelines-created-endpoint\",\n",
    "            )\n",
    "\n",
    "            # deploy the model from Model Registry\n",
    "            ModelDeployOp(\n",
    "                endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "                model=model_upload_op.outputs[\"model\"],\n",
    "                deployed_model_display_name=display_name,\n",
    "                traffic_split = {'0':100},\n",
    "                dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "                dedicated_resources_accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "                dedicated_resources_accelerator_count=1,\n",
    "                dedicated_resources_min_replica_count=1,\n",
    "                dedicated_resources_max_replica_count=1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline"
   },
   "source": [
    "## Compile the pipeline\n",
    "\n",
    "Next, compile the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"facemask classification pipeline.json\".replace(\" \", \"_\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:automl,image"
   },
   "source": [
    "## Run the pipeline\n",
    "\n",
    "Next, run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220624040326\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220624040326')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-east1/pipelines/runs/face-mask-import-model-v1-20220624040326?project=591661299323\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220624040326 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220624040326 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "pipeline_params = {\n",
    "    \"display_name\": DISPLAY_NAME,\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"region\": REGION,\n",
    "    \"bucket\": BUCKET_URI,\n",
    "    \"bucket_name\": BUCKET_URI.replace('gs://', ''),\n",
    "    \"divergence_threshold\": DIVERGENCE_THRESHOLD,\n",
    "    \"container_img_uri\": CONTAINER_IMG_URI,\n",
    "    \"model_perf_uri\": BUCKET_URI + \"/Models/{}\".format(MODEL_PERFORMANCE_FILENAME),\n",
    "    \"eval_threshold\": THRESHOLDS_DICT['f1'],\n",
    "    \"artifact_uri\": MODEL_URI,\n",
    "}\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"facemask classification pipeline.json\".replace(\" \", \"_\"),\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    "    parameter_values=pipeline_params,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:pipelines"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial -- *Note:* this is auto-generated and not all resources may be applicable for this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- Pipeline\n",
    "- Model\n",
    "- Endpoint\n",
    "- Batch Job\n",
    "- Custom Job\n",
    "- Hyperparameter Tuning Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "nHloY5skMEYY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Endpoint : projects/591661299323/locations/us-east1/endpoints/6095446173785522176\n",
      "400 There are other operations running on the Endpoint \"projects/591661299323/locations/us-east1/endpoints/6095446173785522176\". Operation(s) are: projects/591661299323/locations/us-east1/operations/4509212634230292480.\n",
      "Deleting Model : projects/591661299323/locations/us-east1/models/185509601838366720\n",
      "400 The Model \"projects/591661299323/locations/us-east1/models/185509601838366720\" is deployed or being deployed at the following Endpoint(s): projects/591661299323/locations/us-east1/endpoints/6095446173785522176.\n",
      "Deleting PipelineJob : projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220623024310\n",
      "Delete PipelineJob  backing LRO: projects/591661299323/locations/us-east1/operations/2905931166886395904\n",
      "PipelineJob deleted. . Resource name: projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220623024310\n",
      "Deleted pipeline: <google.cloud.aiplatform.pipeline_jobs.PipelineJob object at 0x7fc8215c1710> \n",
      "resource name: projects/591661299323/locations/us-east1/pipelineJobs/face-mask-import-model-v1-20220623024310\n"
     ]
    }
   ],
   "source": [
    "delete_pipeline = True\n",
    "delete_model = True\n",
    "delete_endpoint = True\n",
    "delete_bucket = False\n",
    "\n",
    "try:\n",
    "    if delete_endpoint and \"DISPLAY_NAME\" in globals():\n",
    "        endpoints = aip.Endpoint.list(\n",
    "            order_by=\"create_time\"\n",
    "        )\n",
    "        endpoint = endpoints[0]\n",
    "        endpoint.undeploy_all()\n",
    "        aip.Endpoint.delete(endpoint, force=True)\n",
    "        print(\"Deleted endpoint:\", endpoint)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    if delete_model and \"DISPLAY_NAME\" in globals():\n",
    "        models = aip.Model.list(\n",
    "            filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "        )\n",
    "        model = models[0]\n",
    "        aip.Model.delete(model)\n",
    "        print(\"Deleted model:\", model)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    if delete_pipeline and \"DISPLAY_NAME\" in globals():\n",
    "        pipelines = aip.PipelineJob.list(\n",
    "            filter=f\"display_name={DISPLAY_NAME}\", order_by=\"create_time\"\n",
    "        )\n",
    "        pipeline = pipelines[0]\n",
    "        aip.PipelineJob.delete(pipeline)\n",
    "        print(\"Deleted pipeline:\", pipeline)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "####### BE CAREFUL OF THIS! DON'T RUN!!!!!!!!!!!\n",
    "####### BE CAREFUL OF THIS! DON'T RUN!!!!!!!!!!!\n",
    "####### BE CAREFUL OF THIS! DON'T RUN!!!!!!!!!!!\n",
    "######## if delete_bucket and \"GCS_BUCKET\" in globals():\n",
    "########     ! gsutil rm -r $GCS_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "google_cloud_pipeline_components_automl_images.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
