{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02177b13-a831-4a51-8b83-be1bb8a657b2",
   "metadata": {},
   "source": [
    "# Create Container to Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1fdd8f-aaee-4047-a64b-7d0f4b432920",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ./daily_generate\n",
    "cat > ./daily_generate/generatedata.py <<CODE\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "BaseDay=datetime.datetime(2022, 6, 15, 1, 58, 45, 476805)\n",
    "Today=datetime.datetime.now()\n",
    "diff=(Today-BaseDay).days\n",
    "ds=str(Today)[:10]\n",
    "\n",
    "model_dir='gs://seangoh-smu-mle-usa/Models/FaceMaskEfficientNetModel'\n",
    "old_data_dir='gs://seangoh-smu-mle-usa/FaceMask/Test'\n",
    "drift_data_dir='gs://seangoh-smu-mle-usa/FaceMask/Drift'\n",
    "#os.system('mkdir -p ./app')\n",
    "os.system('mkdir -p /app/{}/WithMask'.format(ds))\n",
    "os.system('mkdir -p /app/{}/WithoutMask'.format(ds))\n",
    "os.system('gsutil -mq cp -r {} /app'.format(model_dir)) #remove . for docker image\n",
    "os.system('gsutil -mq cp -r {} /app'.format(old_data_dir))\n",
    "os.system('gsutil -mq cp -r {} /app'.format(drift_data_dir))\n",
    "\n",
    "DriftMaskDir='/app/Drift/WithMask'\n",
    "DriftNoMaskDir='/app/Drift/WithoutMask'\n",
    "OriginalMaskDir='/app/Test/WithMask'\n",
    "OriginalNoMaskDir='/app/Test/WithoutMask'\n",
    "DestinationWithMask='/app/{}/WithMask'.format(ds)\n",
    "DestinationWithoutMask='/app/{}/WithoutMask'.format(ds)\n",
    "CopyFrom='/app/{}'.format(ds)\n",
    "\n",
    "import random\n",
    "#Change Period Elapsed to Variable\n",
    "PeriodElapsed=diff\n",
    "DataDecayRatio=0.9\n",
    "DataSizeTrend=1.1\n",
    "DistributionDrift=0.9\n",
    "BaseTotalSample=100\n",
    "TotalSample=BaseTotalSample*(0.7+0.6*(random.random())*DataSizeTrend**PeriodElapsed)\n",
    "CurrentDriftRatio=1-DataDecayRatio**(PeriodElapsed+random.random()/2)\n",
    "CurrentWithoutMaskRatio=1-0.5*DistributionDrift**(PeriodElapsed-1+random.random()/2)\n",
    "\n",
    "DriftSampleSize=int(TotalSample*CurrentDriftRatio)\n",
    "OriginalSampleSize=int(TotalSample-DriftSampleSize)\n",
    "\n",
    "DriftWithoutMask=int(CurrentWithoutMaskRatio*DriftSampleSize)\n",
    "DriftWithMask=int(DriftSampleSize-DriftWithoutMask)\n",
    "OriginalWithoutMask=int(CurrentWithoutMaskRatio*OriginalSampleSize)\n",
    "OriginalWithMask=int(OriginalSampleSize-OriginalWithoutMask)\n",
    "\n",
    "\n",
    "\n",
    "DriftMaskNum=len(os.listdir(DriftMaskDir))\n",
    "DriftNoMaskNum=len(os.listdir(DriftNoMaskDir))\n",
    "OriginalMaskNum=len(os.listdir(OriginalMaskDir))\n",
    "OriginalNoMaskNum=len(os.listdir(OriginalNoMaskDir))\n",
    "\n",
    "DMSample,DMRepeat=DriftWithMask%DriftMaskNum,DriftWithMask//DriftMaskNum\n",
    "DNMSample,DNMRepeat=DriftWithoutMask%DriftNoMaskNum,DriftWithoutMask//DriftNoMaskNum\n",
    "OMSample,OMRepeat=OriginalWithMask%OriginalMaskNum,OriginalWithMask//OriginalMaskNum\n",
    "ONMSample,ONMRepeat=OriginalWithoutMask%OriginalNoMaskNum,OriginalWithoutMask//OriginalNoMaskNum\n",
    "\n",
    "def GeneratePaths(datadir,repeat,sample):\n",
    "    dirs=os.listdir(datadir)\n",
    "    paths=[os.path.join(datadir,imagedir) for imagedir in dirs*repeat]\n",
    "    sampleindex=random.sample(dirs,sample)\n",
    "    newpaths=[os.path.join(datadir,i) for i in sampleindex]\n",
    "    paths.extend(newpaths)\n",
    "    return paths\n",
    "\n",
    "#Generate Directories to Copy\n",
    "DriftMaskPaths=GeneratePaths(DriftMaskDir,DMRepeat,DMSample)\n",
    "DriftNoMaskPaths=GeneratePaths(DriftNoMaskDir,DNMRepeat,DNMSample)\n",
    "OriginalMaskPaths=GeneratePaths(OriginalMaskDir,OMRepeat,OMSample)\n",
    "OriginalNoMaskPaths=GeneratePaths(OriginalNoMaskDir,ONMRepeat,ONMSample)\n",
    "\n",
    "#Copy Files\n",
    "import shutil\n",
    "def GenerateFiles(filepaths,folderdestination,prefix):\n",
    "    for num,path in enumerate(filepaths):\n",
    "        destination=os.path.join(folderdestination,prefix+str(num)+'.jpeg')\n",
    "        shutil.copyfile(path,destination)\n",
    "    return\n",
    "\n",
    "\n",
    "GenerateFiles(DriftMaskPaths,DestinationWithMask,'D')\n",
    "GenerateFiles(DriftNoMaskPaths,DestinationWithoutMask,'DN')\n",
    "GenerateFiles(OriginalMaskPaths,DestinationWithMask,'O')\n",
    "GenerateFiles(OriginalNoMaskPaths,DestinationWithoutMask,'ON')\n",
    "\n",
    "#os.system('gsutil rm gs://seangoh-smu-mle-usa/DailyQC/{}'.format(ds))\n",
    "shellcommand='gsutil -mq cp -r {} gs://seangoh-smu-mle-usa/DailyQC/'.format(CopyFrom,ds)\n",
    "os.system(shellcommand)\n",
    "CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b348342-9359-400e-ad88-1d87a882ed24",
   "metadata": {},
   "source": [
    "Create Docker File and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192acb9e-8b3f-4fc0-bdf4-d807d9e6b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./daily_generate/Dockerfile <<EOF\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "WORKDIR /app\n",
    "\n",
    "COPY . /app\n",
    "\n",
    "ENTRYPOINT [\"python\", \"generatedata.py\"]\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b2fad4-dfa3-425d-99aa-3f345c6aa965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      " ---> 4d10005d4e6f\n",
      "Step 2/4 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> f92293ea174a\n",
      "Step 3/4 : COPY . /app\n",
      " ---> Using cache\n",
      " ---> ec9141899947\n",
      "Step 4/4 : ENTRYPOINT [\"python\", \"generatedata.py\"]\n",
      " ---> Using cache\n",
      " ---> 14debabcc583\n",
      "Successfully built 14debabcc583\n",
      "Successfully tagged masketeers/generatedata:latest\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker build ./daily_generate/ -t masketeers/generatedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f462c649-649d-4476-b8a3-1f02e17ec66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run masketeers/generatedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68a751-6976-4c64-a8be-92fb18f7b099",
   "metadata": {},
   "source": [
    "Check that data is successfully created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ce1b87-635b-42a5-8b27-a9c51f186c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-15/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-16/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-17/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-18/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-19/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-20/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-21/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-22/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-23/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-24/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-25/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-26/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-27/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-27/WithMask/\n",
      "gs://seangoh-smu-mle-usa/DailyQC/2022-06-27/WithoutMask/\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "Today=datetime.datetime.now()\n",
    "ds=str(Today)[:10]\n",
    "\n",
    "!gsutil ls gs://seangoh-smu-mle-usa/DailyQC\n",
    "!gsutil ls gs://seangoh-smu-mle-usa/DailyQC/$ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90abf3af-d1f7-451e-bf45-0f28bed8dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      " ---> 4d10005d4e6f\n",
      "Step 2/4 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> f92293ea174a\n",
      "Step 3/4 : COPY . /app\n",
      " ---> Using cache\n",
      " ---> ec9141899947\n",
      "Step 4/4 : ENTRYPOINT [\"python\", \"generatedata.py\"]\n",
      " ---> Using cache\n",
      " ---> 14debabcc583\n",
      "Successfully built 14debabcc583\n",
      "Successfully tagged us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/generatedata:latest\n",
      "The push refers to repository [us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/generatedata]\n",
      "\n",
      "\u001b[1B47bfb344: Preparing \n",
      "\u001b[1B6e8edcb8: Preparing \n",
      "\u001b[1B37ed3ebe: Preparing \n",
      "\u001b[1B0c97e748: Preparing \n",
      "\u001b[1B22d8d85c: Preparing \n",
      "\u001b[1Bb01c5179: Preparing \n",
      "\u001b[1Be696ff5b: Preparing \n",
      "\u001b[1B43fff4a9: Preparing \n",
      "\u001b[1B9c14a32d: Preparing \n",
      "\u001b[1Bd198ee7d: Preparing \n",
      "\u001b[1B46a1d19e: Preparing \n",
      "\u001b[1Bb83c1c63: Preparing \n",
      "\u001b[1Bc31d7bf8: Preparing \n",
      "\u001b[1B03bffb47: Preparing \n",
      "\u001b[1Bd4d6cdf8: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1Bc4cf558f: Preparing \n",
      "\u001b[3Bbf18a086: Preparing \n",
      "\u001b[1B6f75faab: Preparing \n",
      "\u001b[1Bedc62fb3: Layer already exists \u001b[20A\u001b[2K\u001b[15A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2Klatest: digest: sha256:cbd46c5d2b8fd9683a5fc53d76370ee0d04c5f43459119857aa776d72c38606b size: 4701\n"
     ]
    }
   ],
   "source": [
    "!docker build ./daily_generate/ -t us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/generatedata:latest\n",
    "!docker push us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/generatedata:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64aadea-03f4-48f1-9bfb-043925cda317",
   "metadata": {},
   "source": [
    "# Daily Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c93d203-c317-458f-8c59-b28c138a3b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ./daily_predict\n",
    "\n",
    "cat > ./daily_predict/daily_predict.py <<CODE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "#from PIL import Image\n",
    "import os\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "#get daily directory name\n",
    "Today=datetime.datetime.now()\n",
    "ds=str(Today)[:10]\n",
    "dailydatadir='gs://seangoh-smu-mle-usa/DailyQC/{}'.format(ds)\n",
    "\n",
    "\n",
    "#download data\n",
    "os.system('gsutil -mq cp -r {} /app/'.format(dailydatadir))\n",
    "#download model\n",
    "os.system('gsutil cp -r gs://seangoh-smu-mle-usa/Models/FaceMaskEfficientNetModel /app/')\n",
    "\n",
    "dailydirectory=\"/app/{}/\".format(ds)\n",
    "image_size=224\n",
    "ValidationData=keras.utils.image_dataset_from_directory(dailydirectory, class_names=[\"WithoutMask\",\"WithMask\"], image_size=(image_size,image_size))\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = keras.models.load_model('FaceMaskEfficientNetModel')\n",
    "          \n",
    "import numpy as np\n",
    "all_label=[]\n",
    "all_pred=[]\n",
    "for data,labels in ValidationData:\n",
    "    all_label.extend(list(labels))\n",
    "    probs=model.predict(data)\n",
    "    preds=np.argmax(probs, axis=1)\n",
    "    all_pred.extend(list(preds))\n",
    "          \n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "f1=float(f1_score(all_pred,all_label))\n",
    "accuracy=float(accuracy_score(all_pred,all_label))\n",
    "predict_1=int(np.sum(all_pred))\n",
    "predict_0=int(np.sum([1 for i in all_pred if i==0]))\n",
    "label_1=int(np.sum(all_label))\n",
    "label_0=int(np.sum([1 for i in all_label if i==0]))\n",
    "predict_1_percent=float(predict_1/(predict_0+predict_1))\n",
    "predict_0_percent=float(predict_0/(predict_0+predict_1))\n",
    "label_1_percent=float(label_1/(label_0+label_1))\n",
    "label_0_percent=float(label_0/(label_0+label_1))\n",
    "\n",
    "#creat json\n",
    "#dailyperformance={'accuracy':accuracy, 'f1':f1}\n",
    "#import json\n",
    "#with open('dailyperformance.json', 'w') as f:\n",
    "#    json.dump(dailyperformance, f)\n",
    "#os.system('gsutil cp dailyperformance.json gs://seangoh-smu-mle-usa/testupload/')\n",
    "    \n",
    "#upload bq    \n",
    "os.system('gcloud auth activate-service-account --key-file=daring-hash-348101-84e938ac4698.json')\n",
    "print(\"prediction_finished\")\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    'daring-hash-348101-84e938ac4698.json', scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "client = bigquery.Client(credentials=credentials, project=credentials.project_id,)\n",
    "dataset_id='facemask'\n",
    "schematable_id='dailyqc_results'\n",
    "table_ref=client.dataset(dataset_id).table(schematable_id)\n",
    "table=client.get_table(table_ref)\n",
    "row_insert=[(ds,f1,accuracy,predict_1,predict_0,label_1,label_0,predict_1_percent,predict_0_percent,label_1_percent,label_0_percent)]\n",
    "client.insert_rows(table,row_insert)\n",
    "\n",
    "\n",
    "CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d4c9427-a810-48f4-94bf-acdccde4b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./daily_predict/Dockerfile <<EOF\n",
    "FROM gcr.io/deeplearning-platform-release/tf-gpu.2-8\n",
    "WORKDIR /app\n",
    "\n",
    "COPY . /app\n",
    "\n",
    "ENTRYPOINT [\"python\", \"daily_predict.py\"]\n",
    "EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "301f9adc-d102-418c-82d5-b5170e3ef39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp daring-hash-348101-84e938ac4698.json ./daily_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bf2449b-b3d6-433d-9f57-f65874f71b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  15.87kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/tf-gpu.2-8\n",
      " ---> cc037125fdd9\n",
      "Step 2/4 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> 884c3492a942\n",
      "Step 3/4 : COPY . /app\n",
      " ---> 9c273fe9274e\n",
      "Step 4/4 : ENTRYPOINT [\"python\", \"daily_predict.py\"]\n",
      " ---> Running in fe73fc405ae8\n",
      "Removing intermediate container fe73fc405ae8\n",
      " ---> 7db12a193d8e\n",
      "Successfully built 7db12a193d8e\n",
      "Successfully tagged masketeers/dailypredict:latest\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker build ./daily_predict/ -t masketeers/dailypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c656bc-d045-4021-a970-bc842d896d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://seangoh-smu-mle-usa/Models/FaceMaskEfficientNetModel/keras_metadata.pb...\n",
      "Copying gs://seangoh-smu-mle-usa/Models/FaceMaskEfficientNetModel/saved_model.pb...\n",
      "Copying gs://seangoh-smu-mle-usa/Models/FaceMaskEfficientNetModel/variables/variables.data-00000-of-00001...\n",
      "Copying gs://seangoh-smu-mle-usa/Models/FaceMaskEfficientNetModel/variables/variables.index...\n",
      "/ [4 files][ 20.5 MiB/ 20.5 MiB]                                                \n",
      "Operation completed over 4 objects/20.5 MiB.                                     \n",
      "2022-06-27 14:07:26.629864: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2022-06-27 14:07:26.629962: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-27 14:07:26.629997: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (401a455a5413): /proc/driver/nvidia/version does not exist\n",
      "2022-06-27 14:07:26.694289: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "2022-06-27 14:07:45.469350: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_33792\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:12\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2022-06-27 14:07:45.607381: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-27 14:07:51.090261: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_36052\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:35\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2022-06-27 14:07:52.141926: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_36314\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:58\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2022-06-27 14:07:53.193333: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_36576\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:81\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2022-06-27 14:07:54.247329: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_36838\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:104\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2022-06-27 14:07:55.277210: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_37100\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:127\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2022-06-27 14:07:56.295799: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_37303\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022FlatMapDataset:150\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "Activated service account credentials for: [591661299323-compute@developer.gserviceaccount.com]\n",
      "Found 208 files belonging to 2 classes.\n",
      "prediction_finished\n"
     ]
    }
   ],
   "source": [
    "!docker run masketeers/dailypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44ad493f-c5d8-43cd-a9fd-f41f66d7f0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  15.87kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/tf-gpu.2-8\n",
      " ---> cc037125fdd9\n",
      "Step 2/4 : WORKDIR /app\n",
      " ---> Using cache\n",
      " ---> 884c3492a942\n",
      "Step 3/4 : COPY . /app\n",
      " ---> Using cache\n",
      " ---> 9c273fe9274e\n",
      "Step 4/4 : ENTRYPOINT [\"python\", \"daily_predict.py\"]\n",
      " ---> Using cache\n",
      " ---> 7db12a193d8e\n",
      "Successfully built 7db12a193d8e\n",
      "Successfully tagged us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/dailypredict:latest\n",
      "The push refers to repository [us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/dailypredict]\n",
      "\n",
      "\u001b[1Bff899eec: Preparing \n",
      "\u001b[1B95a9cd9c: Preparing \n",
      "\u001b[1Bb3120056: Preparing \n",
      "\u001b[1B53b734c4: Preparing \n",
      "\u001b[1Bb4a993bf: Preparing \n",
      "\u001b[1Ba0734f1b: Preparing \n",
      "\u001b[1B39fb5680: Preparing \n",
      "\u001b[1B6da164cd: Preparing \n",
      "\u001b[1B22d8d85c: Preparing \n",
      "\u001b[1Bb01c5179: Preparing \n",
      "\u001b[1Be696ff5b: Preparing \n",
      "\u001b[1B43fff4a9: Preparing \n",
      "\u001b[1B86de6044: Preparing \n",
      "\u001b[1B8d193daf: Preparing \n",
      "\u001b[1B188023c9: Preparing \n",
      "\u001b[1B0496c2b3: Preparing \n",
      "\u001b[1Bdc387b12: Preparing \n",
      "\u001b[1B257dc2e4: Preparing \n",
      "\u001b[1B54032850: Preparing \n",
      "\u001b[1B951137ff: Preparing \n",
      "\u001b[1Bc25e1d03: Preparing \n",
      "\u001b[1B01bb0f15: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B6f75faab: Preparing \n",
      "\u001b[1Bac543081: Preparing \n",
      "\u001b[1Bc4c62eef: Preparing \n",
      "\u001b[1Ba71261c7: Preparing \n",
      "\u001b[1Bba43cdbe: Preparing \n",
      "\u001b[1B942867a5: Preparing \n",
      "\u001b[1Bfe6d10a9: Preparing \n",
      "\u001b[1B91182163: Preparing \n",
      "\u001b[1B6c5bb65c: Preparing \n",
      "\u001b[1B550a3bbe: Preparing \n",
      "\u001b[1Bedc62fb3: Layer already exists kB\u001b[34A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[24A\u001b[2K\u001b[18A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[3A\u001b[2Klatest: digest: sha256:f70bb99ad0f0d68814e801fe56b3948b6bf00f64e4b00d8d7951756436612a9c size: 7456\n"
     ]
    }
   ],
   "source": [
    "!docker build ./daily_predict/ -t us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/dailypredict:latest\n",
    "!docker push us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/dailypredict:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f398e8f1-936a-4e43-8440-81098f514ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gsutil rm -r gs://seangoh-smu-mle-usa/DailyQC/$ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e482ab-ee21-40cd-884c-cb28f57dfc0a",
   "metadata": {},
   "source": [
    "# Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "429b3cc5-6438-4562-a256-8d295fe941d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import google.cloud.aiplatform as aip\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (\n",
    "    Dataset, Model, Output, Input, \n",
    "    OutputPath, ClassificationMetrics, Metrics, component\n",
    ")\n",
    "import kfp.v2\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "DISPLAY_NAME='facemask'\n",
    "PROJECT_ID='daring-hash-348101'\n",
    "REGION='us-east1'\n",
    "GCS_BUCKET='seangoh-smu-mle-usa'\n",
    "PIPELINE_ROOT='gs://seangoh-smu-mle-usa/projectpipeline'\n",
    "BUCKET_URI='gs://'+GCS_BUCKET\n",
    "\n",
    "@kfp.v2.dsl.pipeline(name=DISPLAY_NAME,pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(project: str = PROJECT_ID, \n",
    "    region: str = REGION, bucket:str = GCS_BUCKET):\n",
    "\n",
    "    generate_data=gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        display_name=DISPLAY_NAME,\n",
    "        container_uri='us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/generatedata:latest',\n",
    "        project=project,\n",
    "        location=region,\n",
    "        staging_bucket=bucket,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        replica_count=1, \n",
    "    ).set_caching_options(False)\n",
    "\n",
    "    evaluate_upload=gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        display_name=DISPLAY_NAME,\n",
    "        container_uri='us-east1-docker.pkg.dev/daring-hash-348101/smu-mle-usa/dailypredict',\n",
    "        project=project,\n",
    "        location=region,\n",
    "        staging_bucket=bucket,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        #accelerator_type='NVIDIA_TESLA_K80',\n",
    "        replica_count=1, \n",
    "    ).after(generate_data).set_caching_options(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f76a4a96-82b6-4f5d-9433-3ceafee9d2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/591661299323/locations/us-east1/pipelineJobs/facemask-20220627140831\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/591661299323/locations/us-east1/pipelineJobs/facemask-20220627140831')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-east1/pipelines/runs/facemask-20220627140831?project=591661299323\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/facemask-20220627140831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/facemask-20220627140831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/facemask-20220627140831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/facemask-20220627140831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/facemask-20220627140831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/facemask-20220627140831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/591661299323/locations/us-east1/pipelineJobs/facemask-20220627140831 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/591661299323/locations/us-east1/pipelineJobs/facemask-20220627140831\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"dailypredict.json\",\n",
    ")\n",
    "\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"dailypredict.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    "    location='us-east1'\n",
    ")\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10809d75-6e09-44ff-b54f-45b82cad409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://dailypredict.json [Content-Type=application/json]...\n",
      "/ [1 files][  6.1 KiB/  6.1 KiB]                                                \n",
      "Operation completed over 1 objects/6.1 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp dailypredict.json gs://seangoh-smu-mle-usa/ProjectPipeline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63b3ce95-fa2f-4b7d-b2fe-23ad8132782e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/google/client/client.py:173: FutureWarning: AIPlatformClient will be deprecated in v2.0.0. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient  # noqa: F811\n",
    "\n",
    "api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)\n",
    "    \n",
    "response = api_client.create_schedule_from_job_spec(\n",
    "    job_spec_path=\"gs://seangoh-smu-mle-usa/ProjectPipeline/dailypredict.json\",\n",
    "    schedule=\"15 15 * * *\",\n",
    "    time_zone=\"America/Indiana/Knox\",  # change this as necessary\n",
    "    parameter_values={'project':PROJECT_ID, 'region':REGION, 'bucket': GCS_BUCKET},\n",
    "    #pipeline_root=PIPELINE_ROOT,  # this argument is necessary if you did not specify PIPELINE_ROOT as part of the pipeline definition.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f212adc-cf66-494e-acc3-ffd2e84b42c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
